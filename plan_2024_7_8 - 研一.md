# (2024.9-研一上)

## 7.7-7.24 py语法基础

- [x] python基础            --7.17
  
  

## 7.25-10.25 基础课程

### 01.李宏毅春季机器学习2021/2022

#### 第一节 Introduction

* ##### **标题**：深度学习介绍

* **作业**：HW1: Regression

* **学习方法论**：主要是基础概念的介绍，快速过一遍。

---

![9103556f7df0a993f6c37d19bafb4ba6](file:///C:/Users/asus/Documents/Tencent%20Files/2716730009/nt_qq/nt_data/Pic/2024-09/Ori/9103556f7df0a993f6c37d19bafb4ba6.png)

![2419e54d-0cd3-4730-b12e-53c19b99cc3f](file:///C:/Users/asus/Pictures/Typedown/2419e54d-0cd3-4730-b12e-53c19b99cc3f.png)

![b16e7d58-47b7-46fc-abbb-9319013b6f16](file:///C:/Users/asus/Pictures/Typedown/b16e7d58-47b7-46fc-abbb-9319013b6f16.png)

![7af671b6-7785-40f5-a591-62b3b81b9dea](file:///C:/Users/asus/Pictures/Typedown/7af671b6-7785-40f5-a591-62b3b81b9dea.png)

---



#### 第二节 Deep Learning

* **标题**：为什么训练网络会失败

* **作业**：HW2: Classification

* **学习方法论**：主要是将训练网络的一些细节，局部最小值，鞍点，自适应学习率，损失函数等。这一讲的选修的梯度下降必看，新的优化器可以先不看，如果有余力可以看，主要讲了对梯度下降的一些改进。

---

01.about the <u>saddle</u>(p19)

The road in the high-dimensional space cannot be seen in the low-dimensional space.

![00deb937-2b0b-4547-855b-117881f6619f](file:///C:/Users/asus/Pictures/Typedown/00deb937-2b0b-4547-855b-117881f6619f.png)

<u>mean that the negative eigen vector is decreased way</u>

![c5305714-e4ed-4b6a-8b76-2953aecfa9db](file:///C:/Users/asus/Pictures/Typedown/c5305714-e4ed-4b6a-8b76-2953aecfa9db.png)

“high dimension may see the different way”

---

02.select the <u>batch</u> and <u>momentum</u> for optim

![c366fae2-0a8e-4aa9-976c-98d74c8160ad](file:///C:/Users/asus/Pictures/Typedown/c366fae2-0a8e-4aa9-976c-98d74c8160ad.png)

<u>Big batch mean update parameters need long time，and small batch mean update parameters need short time.</u>

![c1a59384-9252-4736-af9d-4dfc15387df5](file:///C:/Users/asus/Pictures/Typedown/c1a59384-9252-4736-af9d-4dfc15387df5.png)

![7edd4f59-b8ce-46c8-b50a-52148f728940](file:///C:/Users/asus/Pictures/Typedown/7edd4f59-b8ce-46c8-b50a-52148f728940.png)

![280c032e-7a0f-4684-80de-ab2ed68819bd](file:///C:/Users/asus/Pictures/Typedown/280c032e-7a0f-4684-80de-ab2ed68819bd.png)

 <u>Momentum mainly change the way of GRAD so that it will reduce the probability of local minima.</u>

---

03.adptive learning rate

![6f155315-ae8a-4b50-924b-083e1b85f3b8](file:///C:/Users/asus/Pictures/Typedown/6f155315-ae8a-4b50-924b-083e1b85f3b8.png)

![481d6db4-1932-4693-ae3b-3951e1304f5e](file:///C:/Users/asus/Pictures/Typedown/481d6db4-1932-4693-ae3b-3951e1304f5e.png)

![b32940ff-d678-4e7d-adc1-36fc83e5f3d1](file:///C:/Users/asus/Pictures/Typedown/b32940ff-d678-4e7d-adc1-36fc83e5f3d1.png)

<u>smaller g = larger step,larger g = smaller step</u>

![54c1cee0-05b1-4fa3-b0d5-a6355326d202](file:///C:/Users/asus/Pictures/Typedown/54c1cee0-05b1-4fa3-b0d5-a6355326d202.png)

![2ee20771-6d58-4a93-9c7e-63204e19964c](file:///C:/Users/asus/Pictures/Typedown/2ee20771-6d58-4a93-9c7e-63204e19964c.png)

<mark>Summary:</mark>

![1e9ba941-2a95-4ce4-9f42-359bfa2772ce](file:///C:/Users/asus/Pictures/Typedown/1e9ba941-2a95-4ce4-9f42-359bfa2772ce.png)

---

04.loss function may effect

![4b199d43-b42c-481a-8e98-1b1ca91da598](file:///C:/Users/asus/Pictures/Typedown/4b199d43-b42c-481a-8e98-1b1ca91da598.png)

![d9652cad-ecca-461f-aaad-45409533537c](file:///C:/Users/asus/Pictures/Typedown/d9652cad-ecca-461f-aaad-45409533537c.png)

<u>Generally speaking，softmax and Cross-entropy are tied together</u>

![e268807c-5368-41a1-9408-0e7a170fda7a](file:///C:/Users/asus/Pictures/Typedown/e268807c-5368-41a1-9408-0e7a170fda7a.png)

05.batch nomalization(serendipitous)

goal:transform to same range(input:x)

![efd62fc1-b390-43e3-a9ca-3a447336f8ec](file:///C:/Users/asus/Pictures/Typedown/efd62fc1-b390-43e3-a9ca-3a447336f8ec.png)

![6e3c548d-93be-4283-a629-fe75d9482b72](file:///C:/Users/asus/Pictures/Typedown/6e3c548d-93be-4283-a629-fe75d9482b72.png)

z have different range,then, optimize them

![0a263404-abfe-4f64-bee9-210e27f976e4](file:///C:/Users/asus/Pictures/Typedown/0a263404-abfe-4f64-bee9-210e27f976e4.png)

<img title="" src="file:///C:/Users/asus/Pictures/Typedown/61cea5f9-e52e-4503-9fd2-67fd557c1498.png" alt="61cea5f9-e52e-4503-9fd2-67fd557c1498" style="zoom:100%;">

![aeda90c3-e7a8-4e29-9bb7-751e34b012aa](file:///C:/Users/asus/Pictures/Typedown/aeda90c3-e7a8-4e29-9bb7-751e34b012aa.png)

---



#### 第三节 Self-Attention

* **标题**：图像与序列输入处理

* **作业**：HW3: CNN, HW4: Self-Attention

* **学习方法论**：图像作为输入，CNN网络，必看，非常重要。

---

01.CNN

![ed048435-0235-4f65-8604-8eacc068979d](file:///C:/Users/asus/Pictures/Typedown/ed048435-0235-4f65-8604-8eacc068979d.png)

![46bf2d3e-195c-418e-ad7b-8fb15dd87384](file:///C:/Users/asus/Pictures/Typedown/46bf2d3e-195c-418e-ad7b-8fb15dd87384.png)

<u>Why we need CNN?</u>

<u>Extract global feature into somewhere.</u>

<u>Meanwhile,we take the shared parameters strategy for each Neuron(the same Weight).</u>

<img src="file:///C:/Users/asus/Pictures/Typedown/ff6ad30a-4402-4a7c-8e72-171765c812e1.png" title="" alt="ff6ad30a-4402-4a7c-8e72-171765c812e1" style="zoom:150%;">

02.why we need more hiden layer（deep-learning）？

![ec929503-5fdc-448d-9bde-80ad8bbd29df](file:///C:/Users/asus/Pictures/Typedown/ec929503-5fdc-448d-9bde-80ad8bbd29df.png)

Q:why come to overfitting as H_val's loss is fine?

<u>when the H(validation) become more complex，</u>

<u>the P(D_val is bad) is more proable.</u>

![2fd84358-1a87-482d-a501-e9c17e90538d](file:///C:/Users/asus/Pictures/Typedown/2fd84358-1a87-482d-a501-e9c17e90538d.png)

1.small L

2.small difference between h_train and h_all(test)

![027ec8da-7257-4fba-9002-5b7a8685b8ea](file:///C:/Users/asus/Pictures/Typedown/027ec8da-7257-4fba-9002-5b7a8685b8ea.png)

<img src="file:///C:/Users/asus/Pictures/Typedown/8b0e9386-dc4d-4e28-8b65-fb24709b0a28.png" title="" alt="8b0e9386-dc4d-4e28-8b65-fb24709b0a28" style="zoom:150%;">

![8cb6ec76-87bc-4d06-ab01-5b724c9f66ec](file:///C:/Users/asus/Pictures/Typedown/8cb6ec76-87bc-4d06-ab01-5b724c9f66ec.png)

![b11aa52f-3146-4f95-8eb0-7c1d607a16f2](file:///C:/Users/asus/Pictures/Typedown/b11aa52f-3146-4f95-8eb0-7c1d607a16f2.png)

2k(shallow) vs 2^k(deep)

---

02.spatial transformer layer



we know the CNN has limited transformation invariant.

(such as translation,

it can not process enlargement and rotation)

![0a3c64e0-e18b-4bc6-9217-118bad9f6008](file:///C:/Users/asus/Pictures/Typedown/0a3c64e0-e18b-4bc6-9217-118bad9f6008.png)

![43f9079d-79c7-4f35-844e-1f53a75f8a3e](file:///C:/Users/asus/Pictures/Typedown/43f9079d-79c7-4f35-844e-1f53a75f8a3e.png)

<u>if we take rounding off ,can not apply gradient decrease.</u>

<u>Bilinear interpolation method</u>

![310142d1-e858-4de8-965d-9d8f8d7bcc07](file:///C:/Users/asus/Pictures/Typedown/310142d1-e858-4de8-965d-9d8f8d7bcc07.png)

---



#### 第四节 Theory of ML

* **标题**：机器学习理论

* **作业**：无特定作业提及

* **学习方法论**：**序列作为输入，先看选修的RNN，再去看自注意力机制，不要搞错顺序**。
  因为注意力太火了，所以RNN放在了选修，不过我认为还是要先看RNN模型基础，再去看自注意力机制，为下面的Transformer模型做准备。选修中的GNN网络，可以根据自己的需求，入门阶段可以先跳过不看。

---

01.RNN

<u>Changing the sequence order will change the output.</u>

![dac1ea50-2578-46c3-a99b-add47bb528b0](file:///C:/Users/asus/Pictures/Typedown/dac1ea50-2578-46c3-a99b-add47bb528b0.png)

<u>simplfied RNN</u>

![be8965d3-52d5-435a-a3a2-6a3e24effe11](file:///C:/Users/asus/Pictures/Typedown/be8965d3-52d5-435a-a3a2-6a3e24effe11.png)

![500c5293-7de7-4cbf-847b-713381048324](file:///C:/Users/asus/Pictures/Typedown/500c5293-7de7-4cbf-847b-713381048324.png)

![124d369b-3c54-4212-8d9d-0ca53b4de783](file:///C:/Users/asus/Pictures/Typedown/124d369b-3c54-4212-8d9d-0ca53b4de783.png)

![3803fe73-cde4-43a7-9c67-61e9f2230b89](file:///C:/Users/asus/Pictures/Typedown/3803fe73-cde4-43a7-9c67-61e9f2230b89.png)

02.LSTM

<u>LSTM，4 inputs，1 output</u>

![f93f04ff-1794-4e9f-ba8b-ef8f8fccbfb4](file:///C:/Users/asus/Pictures/Typedown/f93f04ff-1794-4e9f-ba8b-ef8f8fccbfb4.png)

<u>Zi，Zf，Zo     0<#<1   apply in linear parameters</u>

![fff9192e-f085-492b-81e7-0813ca1e3032](file:///C:/Users/asus/Pictures/Typedown/fff9192e-f085-492b-81e7-0813ca1e3032.png)

<u>use the latest output Yt,Xt and Ct in next caculate.</u>

![bcd66d0e-b350-472f-ae05-cfda28c8af6c](file:///C:/Users/asus/Pictures/Typedown/bcd66d0e-b350-472f-ae05-cfda28c8af6c.png)

<u>grad = min(grad,clipping_number)</u>

![20f84c52-73bf-4823-be62-eaba61e64218](file:///C:/Users/asus/Pictures/Typedown/20f84c52-73bf-4823-be62-eaba61e64218.png)

![6d2b7a6a-0269-4dab-883b-8f59838ab60e](file:///C:/Users/asus/Pictures/Typedown/6d2b7a6a-0269-4dab-883b-8f59838ab60e.png)

03.self-attention_1

<u>sequence as input</u>

![4076f596-7546-465c-8c5f-f63d6b60b84d](file:///C:/Users/asus/Pictures/Typedown/4076f596-7546-465c-8c5f-f63d6b60b84d.png)

![726832c0-f9a7-4cf5-b3f6-dd69340b164c](file:///C:/Users/asus/Pictures/Typedown/726832c0-f9a7-4cf5-b3f6-dd69340b164c.png)

![1de2d704-61d8-40eb-83a5-6d885cfed1b3](file:///C:/Users/asus/Pictures/Typedown/1de2d704-61d8-40eb-83a5-6d885cfed1b3.png)

![d8ea4363-e5d8-4c57-9198-b20fa2e49771](file:///C:/Users/asus/Pictures/Typedown/d8ea4363-e5d8-4c57-9198-b20fa2e49771.png)

![c51603f0-99e9-4d39-98cd-6aa7c410c138](file:///C:/Users/asus/Pictures/Typedown/c51603f0-99e9-4d39-98cd-6aa7c410c138.png)

![8b93ea68-70a7-4374-9c0c-98fd09dd7722](file:///C:/Users/asus/Pictures/Typedown/8b93ea68-70a7-4374-9c0c-98fd09dd7722.png)

<u>the Soft-max can be RELU,Sigmoid...</u>

![62c4b0cb-85f1-4c2c-9ab9-75cff726280d](file:///C:/Users/asus/Pictures/Typedown/62c4b0cb-85f1-4c2c-9ab9-75cff726280d.png)

04.self-attention_2

![9ba45a2b-58ac-47b4-a2cb-8a2f7f3ef390](file:///C:/Users/asus/Pictures/Typedown/9ba45a2b-58ac-47b4-a2cb-8a2f7f3ef390.png)

![fc17a9fc-aabd-4cf1-aa04-52d3d03a7373](file:///C:/Users/asus/Pictures/Typedown/fc17a9fc-aabd-4cf1-aa04-52d3d03a7373.png)

![80bd5eb0-be2e-4422-8b46-944163c5c3c1](file:///C:/Users/asus/Pictures/Typedown/80bd5eb0-be2e-4422-8b46-944163c5c3c1.png)

<u>Muti-head self-attention</u>

![2027c828-3d1d-472b-908d-5a7d19cf78fe](file:///C:/Users/asus/Pictures/Typedown/2027c828-3d1d-472b-908d-5a7d19cf78fe.png)

![445bc985-b2e0-4116-bfda-bcbe2537d69c](file:///C:/Users/asus/Pictures/Typedown/445bc985-b2e0-4116-bfda-bcbe2537d69c.png)

![6ebd1a88-ad66-4d41-9e63-b91ff532f668](file:///C:/Users/asus/Pictures/Typedown/6ebd1a88-ad66-4d41-9e63-b91ff532f668.png)

![1e62581f-7d2c-4f09-bd93-4c2fb35105d9](file:///C:/Users/asus/Pictures/Typedown/1e62581f-7d2c-4f09-bd93-4c2fb35105d9.png)

![5b868fc1-6cb4-473a-8337-e047322481c9](file:///C:/Users/asus/Pictures/Typedown/5b868fc1-6cb4-473a-8337-e047322481c9.png)

---



#### 第五节 Transformer

* **标题**：序列到序列的转换

* **作业**：HW5: Transformer

* **学习方法论**：主要讲了Transformer模型，必看。选修的指针网络可以先不看，如有兴趣和时间，可选择性学习。

---

![dfbe4ff7-695b-45ff-9b20-0e87513e3c2f](file:///C:/Users/asus/Pictures/Typedown/dfbe4ff7-695b-45ff-9b20-0e87513e3c2f.png)

01.transfermer_encoder

![5f00c3b1-e638-41fd-ae70-342f38f88536](file:///C:/Users/asus/Pictures/Typedown/5f00c3b1-e638-41fd-ae70-342f38f88536.png)

![af2ef2f4-6156-49b0-a042-a09544f4f7a9](file:///C:/Users/asus/Pictures/Typedown/af2ef2f4-6156-49b0-a042-a09544f4f7a9.png)

simplify model：

![44763736-3db1-4029-9974-3555bc1d1a59](file:///C:/Users/asus/Pictures/Typedown/44763736-3db1-4029-9974-3555bc1d1a59.png)

original model：

![e7215859-c3fd-4490-bf0a-9818fbad9b09](file:///C:/Users/asus/Pictures/Typedown/e7215859-c3fd-4490-bf0a-9818fbad9b09.png)

![007ce917-75bd-47ba-921e-8049a067f697](file:///C:/Users/asus/Pictures/Typedown/007ce917-75bd-47ba-921e-8049a067f697.png)

02.transfermer_decoder

how to stop the auto ?

![e74d9c30-b585-4c65-bf98-3411a9031acc](file:///C:/Users/asus/Pictures/Typedown/e74d9c30-b585-4c65-bf98-3411a9031acc.png)

another way to general a sequence(no auto)

![715ee41a-1af0-4dab-bed0-f90904cac886](file:///C:/Users/asus/Pictures/Typedown/715ee41a-1af0-4dab-bed0-f90904cac886.png)

the different encoder&decoder model

![745894c6-8401-4fc6-9369-d7d113ee8b75](file:///C:/Users/asus/Pictures/Typedown/745894c6-8401-4fc6-9369-d7d113ee8b75.png)

crossentropy for train optimizing,

BLEU score for test optimizing 

![cddfc83a-765c-4dbc-a9df-5e2d800025dd](file:///C:/Users/asus/Pictures/Typedown/cddfc83a-765c-4dbc-a9df-5e2d800025dd.png)

some ways to solute serendipity

![95eb0db8-c05e-4b8d-ac0f-4faf316c10f4](file:///C:/Users/asus/Pictures/Typedown/95eb0db8-c05e-4b8d-ac0f-4faf316c10f4.png)

or add some error data in test data，then greater？？？

---



#### 第六节 Generative Model

* **标题**：生成模型

* **作业**：HW6: GAN

* **学习方法论**：主要是对GAN理论的介绍。根据自己的研究方向，如果是GAN方向的，可以细细看下；如果是入门选手，直接跳过也不影响后续学习。

#### 第七节 Self-Supervised Learning

* **标题**：自监督学习

* **作业**：HW7: BERT, HW8: Autoencoder

* **学习方法论**：必看，很火。主要看关于BERT介绍相关的视频，比如模型介绍、微调、预训练等。BERT的各种变体比如Spanbert等可以先不看，没余力直接跳过，不影响后续学习



### 02.吴恩达机器学习



### 03.概率论



### 04.统计学原理



### 05.李沐论文带读



## 9.10-10.20 code-of-learn-dl-with-pytorch

### 课程目录

#### part1: 深度学习基础

##### Chapter 2: PyTorch基础

- [Tensor和Variable](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter2_PyTorch-Basics/Tensor-and-Variable.ipynb)    
- [自动求导机制](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter2_PyTorch-Basics/autograd.ipynb)
- [动态图与静态图](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter2_PyTorch-Basics/dynamic-graph.ipynb)

##### Chapter 3: 神经网络

- [线性模型与梯度下降](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/linear-regression-gradient-descend.ipynb)

- [Logistic 回归与优化器](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/logistic-regression/logistic-regression.ipynb)

- [多层神经网络，Sequential 和 Module](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/nn-sequential-module.ipynb)

- [深层神经网络](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/deep-nn.ipynb)

- [参数初始化方法](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/param_initialize.ipynb)

- 优化算法
  
  - [SGD](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/optimizer/sgd.ipynb)
  - [动量法](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/optimizer/momentum.ipynb)
  - [Adagrad](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/optimizer/adagrad.ipynb)
  - [RMSProp](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/optimizer/rmsprop.ipynb)
  - [Adadelta](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/optimizer/adadelta.ipynb)
  - [Adam](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter3_NN/optimizer/adam.ipynb)

##### Chapter 4: 卷积神经网络

* [PyTorch 中的卷积模块](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/basic_conv.ipynb)
* [批标准化，batch normalization](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/batch-normalization.ipynb)
* [使用重复元素的深度网络，VGG](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/vgg.ipynb)
* [更加丰富化结构的网络，GoogLeNet](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/googlenet.ipynb)
* [深度残差网络，ResNet](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/resnet.ipynb)
* [稠密连接的卷积网络，DenseNet](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/densenet.ipynb)
* 更好的训练卷积网络
  * [数据增强](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/data-augumentation.ipynb)
  * [正则化](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/regularization.ipynb)
  * [学习率衰减](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter4_CNN/lr-decay.ipynb)

##### Chapter 5: 循环神经网络

* [循环神经网络模块：LSTM 和 GRU](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/pytorch-rnn.ipynb)
* [使用 RNN 进行图像分类](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/rnn-for-image.ipynb)
* [使用 RNN 进行时间序列分析](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/time-series/lstm-time-series.ipynb)
* 自然语言处理的应用：
  * [Word Embedding](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/nlp/word-embedding.ipynb)
  * [N-Gram 模型](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/nlp/n-gram.ipynb)
  * [Seq-LSTM 做词性预测](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter5_RNN/nlp/seq-lstm.ipynb)

##### Chapter 6: 生成对抗网络

* [自动编码器](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter6_GAN/autoencoder.ipynb)
* [变分自动编码器](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter6_GAN/vae.ipynb)
* [生成对抗网络](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter6_GAN/gan.ipynb)
* 深度卷积对抗网络 (DCGANs) 生成人脸

##### Chapter 7: 深度强化学习

* [Q Learning](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter7_RL/q-learning-intro.ipynb)
* [Open AI gym](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter7_RL/open_ai_gym.ipynb)
* [Deep Q-networks](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter7_RL/dqn.ipynb)

##### Chapter 8: PyTorch高级

* [tensorboard 可视化](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter8_PyTorch-Advances/tensorboard.ipynb)
  * [灵活的数据读取介绍](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter8_PyTorch-Advances/data-io.ipynb)
* autograd.function 的介绍
* 数据并行和多 GPU
* 使用 ONNX 转化为 Caffe2 模型
* 如何部署训练好的神经网络
* 打造属于自己的 PyTorch 的使用习惯

#### part2: 深度学习的应用

##### Chapter 9: 计算机视觉

- [Fine-tuning: 通过微调进行迁移学习](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter9_Computer-Vision/fine_tune/)
- kaggle初体验:猫狗大战
- [语义分割: 通过 FCN 实现像素级别的分类](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/tree/master/chapter9_Computer-Vision/segmentation)
- Pixel to Pixel 生成对抗网络
- Neural Transfer: 通过卷积网络实现风格迁移
- Deep Dream: 探索卷积网络眼中的世界

##### Chapter 10: 自然语言处理

- [Char RNN 实现文本生成](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch/blob/master/chapter10_Natural-Language-Process/char_rnn/) 
- Image Caption: 实现图片字幕生成
- seq2seq 实现机器翻译
- cnn + rnn + attention 实现文本识别
